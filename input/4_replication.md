# %chapter_number%. Репликация

Проблема репликации одна из многих проблем в распределенных системах. Мы фокусируемся на этой проблеме среди других, таких как выбор лидера(мастер-реплики), определение отказов, взаимоисключения доступа(mutual exclusion), консенсус и глобальные снэпшоты(снимки состояния), потому что зачастую эта проблема интересует большинство людей. Например чтобы иметь возможность различать базы данных по описанию их возможностей в терминах репликации. Кроме того, репликация дает контекст для многих под-проблем таких как выбор лидера, определения отказов, консенсуса и атомарной посылке сообщщений всем узлам сети(броадкастинг).

Репликация это группа связанных проблем. Какие механизмы и шаблоны коммуникации предоставять нам такие показатели производительности и доступности которые нам нужны? Как мы можем предоставлять отказоустойчивость, надежность и отсутсвие расходений сталкиваясь с разделениями сети и одновременными падениями узлом?

Опять таки существует много подходов к репликации. Под подходом здесь понимается просто высокоуровневый шаблон применимый для систем с репликацией. Такой взгляд помогает фокусироватся на общей картине, а не конкретных деталях. Наша цель здесь исследовать простанство возможных дизайнов, а не разобратся с спецификой конкретных алгоритмов.

Для начала дадим определение репликации в общих чертах. Мы предполагаем что мы имеем некотрую базу данных в начальном состоянии, и могут совершать запросы которые изменяют состояние базы данных.

<img src="images/replication-both.png" alt="replication" style="height: 340px;">

Тогда механизм и шаблон коммуникации могут быть разделены на несколько стадий:

1. (Запрос) Клиент посылает запрос на сервер
3. (Синхронная часть) Происходит синхронная часть алгоритма репликации
4. (Результат) Клиенту возврашается ответ
5. (Асинхронная часть) Происходит асинхронная часть репликации

Это стадии выделены по мотивам [этой статьи](https://www.google.com/search?q=understanding+replication+in+databases+and+distributed+systems). Заметим, что модель сообщений которыми обмениваются отдельные части системы в каждой стадии зависит от конкретного алгоритма: попытаемся обойтись без обсуждения конкретных алгоритмов.

Учитывая эти этапы, какие шаблоны коммуникации мы можем создать? И какие неочевидные последствия будут у нашего выбора для производительности и доступности?

## Синхронная репликация

Первый паттерн это синхронная репликация(также известная как активная, или интенсивная или пуш-репликация, или пессимистическая репликация). Давайте изобразим это:

<img src="images/replication-sync.png" alt="replication" style="height: 340px;">

Здесь, мы можем увидеть три различных стадии: первая, клиент отправляет запрос. Следующая, когда выполняется синхронная часть репликации. Во время этой стадии клиент блокируется - ожидая ответ от системы.

Во время синхронной фазы, первый сервер контактирует со вторым и ждет пока он не получит ответы от всех остальных серверов. В конце, он отсылает ответ клиенту информирующий его об успехе или неудачи.

Это выглядит довольно просто. Что мы можем сказать о специфичных соглашениях о шаблонах коммуникации, не погружаясь в детали конкретного алгоритма синхронной фазы? Во-первых, заметим что запись осуществляется при помощи N - из - N подхода: прежде чем результат будет возврашен, он должен быть доставлен и признан корректным каждым сервером в системе(если серверов N то запись должна быть потверждена N раз).

С точки зрения производительности это означает, что система будет быстрой настолько же как и самая медленная машина в кластере. Система также будет крайне чувствительна к изменениям сетевых задержек, так как необходимо дождатся пока ответа каждого сервера прежде чем ответить клиенту.

Использующая данный подход(N-из-N) система не может быть устойчива к выходу из строя какого либо сервера. При падении сервера, система на долгое время теряет возможность продолжать запись на всех узлах и соотвественно не может продолжать обрабатывать запросы клиента. Она все еще может предоставлять доступ к данным на чтение, но их модификация становится не возможно при падении узла при такой архитектуре.

Такое механизм работы позволяет предоставлять строгие гарантии надежности: клиент может быть уверен что все N серверов получили, сохранили и согласились с запросом на модификацию, когда клиент получил ответ. Чтобы потерять принятое обновление необходимо чтобы его потеряли все N серверов, что является настолько хорошей гарантией насколько мы можем дать.

## Асинхронная репликация

Давайте сравним данные подход с другим паттерном - асинхронной репликацией (a.k.a. пассивная репликация, или pull-репликация, или ленивая репликация). Как вы можете догадатся, этот подход противоположен синхронной репликации:

<img src="images/replication-async.png" alt="replication" style="height: 340px;">

Здесь, мастер (/ лидер / координатор) немедленно возврашает ответ клиенту. Максимум что он делает это сохраняет изменения локально, но он не будет делать какую либо синхронную отсылку этих изменений другим серверам и не будет заставлять ждать клиента пока произойдет коммуникация с другими серверами.

После этого, наступает стадия в который выполняется асинхронная часть механизма репликации. Во время нее, мастер сообщает другим серверам обновить локальную версию данных. Детали этого зависят от алгоритма используемого для репликации.

Что мы можем сказать о специфики такого подхода без углубления в детали конкретных алгоритмов? Что ж, это запись 1 - из - N: результат будет возврашен немедленно, а обновления будут распространены немного позже.

С точки зрения производительности это означает что система будет быстрой: клиенту нет нужды тратить время ожидая пока система совершит всю внутреннюю работу. Такая система также более невосприимчива к сетевым задержкам, так как изменения задержек внутри системы не вызывают увеличение времени ожидания клиента.

Такой механизм может предоставлять только слабые или вероятностные гарантии надежности. Если не случится ничего неправильного, тогда данные в конечном итоге будут реплицированны на все N машин. Однако, если только сервер содержаший данные потеряет их прежде чем это случится тогда мы можем потерять эти данные навсегда.

Данный подход, позволяет системе оставатся доступной до тех пор пока остается хотя бы один узел(это только в теории - на практике нагрузка на него будет слишком высока). Полностью "ленивый" подход не предоставляет гарантий надежности и согласованности; вы можете писать данные в систему, но нет никаких гарантий что вы сможете прочитать те данные что вы записали в случае возникновения каких либо ошибок.

В конце концов, следует заметить что пассивная репликация не может обеспечивать одинаковое состояние на всех узлах. Если вы проводите запись в нескольких узлах и не требуете чтобы все узлы были синхронно согласны на запись, вы рискуете получить расхождение данных: чтение может возврашать разные данные из разных источников(в частности для узлов после востановления), и глобальные ограничения(которые требуют коммуникации всех узлов) не могут использоватся при таком подходе.

Детали коммуникации при чтении данных не упомянуты специально, потому что то как вы читаете данные напрямую зависит от способа записи: во время чтения вы хотите контактировать с несколькими узлами - если это представляется возможным. Немного более подробно об этом будет сказано в контексте кворумов.

Мы только обсудили два базовых подхода и не вдавались в детали специфики  конкретных алгоритмов. Еще мы смогли выяснить совсем немного о возможных моделях коммуникации а также их производительности, гарантиях надежности и доступности.

## Обзор большинства способов репликации

У нас была дискуссия о двух главных способах: синхронной и асинхронной репликации, а теперь давайте взглянем на большинство алгоритмов репликации.

Существует очень много различных способов разбить на категории техники применяемые для репликации. Второе разделение(после синхронной и асинхронной) которое мы обсудим, это разделение между:

- Методами репликации которые предотвращают расхождение ("single copy" системы) и
- Методами репликации которые могут приводит к расхождениям (мульти-мастер системы)

Первая группа методов удовлетворяет условиям "поведения аналогичного для не распределенной системы".  В частности, когда отказываает часть системы, гарантируется что только одная копия системы остается активной. Кроме того, система гарантирует что реплики всегда находятся в согласии. Это известно как проблема консенсуса.

Несколько процессов(или компьютеров) достигают консенсуса если все они согласны насчет некотрого значения. Более формально:

1. Соглашение: Каждый корректный процесс должен быть согласен с одним и тем же значением что и другие.
2. Целостность: Каждый корректный процесс может выбрать не более одного значения, и если он выбирает некотрое значение то оно должно быть предложено некотрым процессом.
3. Завершение: Все процессы должны в конечном счете принять решение.
4. Обоснованность: Если все корректные процессы предлагают значение V, тогда все корректные процессы должны принять значение V.

Конкурентный доступ к данным, выбор лидера, атомарная отсылка сообщей всем(броадкаст) или нескольким(мультикаст) узлам - все это частный случаи более общей проблемы консенсуса. Системы репликации которые поддерживат консистетность уровня "одна активная копия"("single-copy")  нуждаются в решении проблемы консенсуса каким либо способом.

Алгоритмы репликации которые поддерживают single-copy согласованность включают в себя следующие категории:

- 1n сообщений (асинхронный primary/backup)
- 2n сообщейний (синхронный primary/backup)
- 4n сообщений (2-ухфазный коммит, Мульти-Паксос)
- 6n сообщений (3-хфазный коммит, Паксос с повторящимся выбором лидера)

Эти алгоритмы отличаются по устойчивости к отказам (то есть типам отказов которые они могут обрабатывать). Классификация выше делит их просто по числу сообщений которые им надо отправить во время выполнения алгоритма, потому что, мне кажется интересно найти ответ на вопрос "Что мы получаем с увеличением обмена сообщениями?"

Диаграмма ниже адаптированная из работы Ryan Barret из [Google](http://www.google.com/events/io/2009/sessions/TransactionsAcrossDatacenters.html), описывает некотрые аспекты возможных вариантов опций:

![Сравнение методов репликации из http://www.google.com/events/io/2009/sessions/TransactionsAcrossDatacenters.html](images/google-transact09.png)

Характеристики согласованности, задержек, пропускной способности, возможности потери данных и способности к востановлению на диаграмме выше могут быть вытекают из сути двух главных методов репликации: синхронная репликация(во время который мы ждем прежде ответить) и асинхронной. Когда мы ждем, мы получаем хуже производительность но выше гарантии надежности. Разница в пропускной способности между 2PC  системах и основанных на кворуме системах станет очевидной когда мы будем обсуждать устойчивость к разделению(и задержкам).


На диаграмме, алгоритмы обеспечивающие слабую согласованность("согласованность в конечном итоге") объединены в одну категорию ("gossip"). Однако, мы будем обсуждать методы репликации с слабой согласованностью - gossip и системы с частичным кворумом - более детально. Строчка с заголовков "transactions" на самом деле ссылается больше на вычисление ограничений в рамках всей системы,  которые не поддерживаются в системах с слабой согласованность (хотя локальные ограничения и условия поддерживаются).

Следует отметить что системы предоставляющие гарантии только слабой согласованности требуют менее общих(более специальных) алгоритмов, и многие техники применимы к ним только частично. Системы не связывающие себя обязательством согласованности с единственной активной копией свободны от ограничения, что они должны работать "как на одной машине", их задачи менее очевидны, что в свою очередь позволяет людям сфокусироватся на том чтобы рассуждать о характеристиках которые имеет такая система.

Для примера:

- Клиент-ориентированная согласованность пытается предоставлять более понятные гарантии согласованности при возможных расхождениях данных.
- CRDTs (сходящиеся и коммутативные реплицируемые типы данных) используют  свойства полукольца (ассоциативность, коммутативность, имподентность) опредленных для структур данных основанных на состоянии и операциях.
- Анализ слияний(Confluence analysis) (как в Bloom language) использует информацию о монотонности вычислений максимально используя отсуствия порядка.
- PBS (вероятностные ограничения устаревания - probabilistically bounded staleness) использует симуляцию  и информацию с реальной системы чтобы характеризовать ожидаемое поведение системы с частичным кворумом.

Мы будем обсуждать все эти техники немного позже; для начала давайте посмотрим на алгоритмы репликации которые поддерживают согласованность во всей системе на уровне одной активной копии не допускающей расхождений(single-copy).

## Primary/backup репликация

Primary/backup репликация (также известная как репликация методом копирования мастера, мастер-слейв репликация и рпеликация отправкой логов) возможно наиболее общий из алгоритмов репликации, и самый простой. Все изменения выполненые на главной реплике и лог всех операций (или всех изменений) отправляется по сети остальным репликам. Как мы помним, возможны два варианта:

- изменения отсылаются асинхронно и
- изменения отсылаются синхронно

Синхронный вариант требует два сообщения ("обновление" + "подтверждение получения") в то время как асинхронный вариант только лишь "обновления".

P/B очень общий алгоритм. Для примера, по умолчанию MySQL репликация использует его асинхронный вариант. MongoDB также использует P/B (с некотрыми дополнениями для востанновления в случе отказа мастера). Все операции выполняются на одном мастер сервере, которые затем упорядочиваются в локальный лог и затем асинхронно воспроизводятся на резервной реплике.

Как мы обсуждали раньше в контексте асинхронной репликации, любая подобный алгоритм может предоставить только слабые гарантии надежности. В MySQL репликации это известно как лаг репликации: асинхронное копирование всегда отстает по крайне мере на одну операцию от мастера. Если мастер упадет, когда обновления еще не будут доставлены они будут потеряны.

Синхронный вариант primary/backup репликации обеспечивает сохранение записи на реплике прежде чем результат будет возвращен клиенту - ценой этому будет ожидание ответа всех реплик. Однако, стоит заметить что даже такой вариант может быть предоставлять только слабые гарантии. Рассмотрим следующий сценарий отказа:

- мастер принимает запись и отправляет ее на реплики
- реплики сохраняют ее и сообщают о успешной записи
- и в этот момент мастер падает прежде чем отправляет ответ клиенту

Клиент допускает что запись была не успешна, но реплики совершили запись; если реплика будет выдвинута на роль мастера, эта запись будет некорректна с точки зрения клиента. Может понадобится ручная очистка для синхронизации расходящихся мастера и реплики.

Данный сценарий конечно намеренно упрощен. Все алгоритмы мастер-слейв репликации следуют одним паттернам коммуникации но они отличаются в способности обработке востанновления после отказа, возможности реплики находится в оффлайне длительное время и так далее. Однако, не возможно быть полностью устойчивым к неудачным падениям мастера в такой схеме.

Ключевым в механизмах основанных на мастер-слейв алгоритмах является то что они предлагают гарантии настолько хорошие насколько это возможно(например они могут терять обновления или наоборот получать некорректные апдейты если узел падает в неподходящий момент). Более того, мастер-слейв конфигурации уязвимы к так называемому "split-brain", когда востановление путем выдвижение в качестве мастер сервера одной из реплик после проблем с сетью приводит к тому что некотрое время оказывается активным оба мастера(новый созданный из реплики и старый вернувшийся в строй).

Для предотвращения сбоя в неподходящий момент мы должны добавить еще один раунд обмена сообщениями, прийдя тем самым к протоколу двух-фазного коммита.

## Двух-фазный коммит (2PC)

[Двух-фазный коммит](http://en.wikipedia.org/wiki/Two-phase_commit_protocol) (2PC) это протокол используемый во многих классических реляционных базах данных. Для примера, MySQL Cluster (не путать с обычным MySQL) предоставляет синхронную репликацию используя 2PC. Диаграмма ниже иллюстрирует поток сообщений:

    [ Координатор ] -> Готовы к совершению операции(commit)?     [ Узлы ]
                    <- Да / Нет

    [ Координатор ] -> Завершить / откатить [ Узлы ]
                    <- Результат операции

В первой фазе(голосования),  координатор отсылает обновления всем участникам. Каждый участвующий процесс исполняет обновления и голосует завершить операции или откатить. Когда голосуют за завершение участники сохраняют обновление в некотрой временной зоне (write-ahead лог). Пока выполняется вторая фаза обновления считаются временными.

Во второй фазе(решения), координатор подводит итог на основе данных с реплик и сообщает каждому участнику. Если все участники голосовали за совершение операции, тогда обновление перемещается из временной зоны в место постоянного хранения.

Наличие второй фазы перед сохранением в постоянное хранилище крайне полезно, так как оно позволяет системе откатится в случае если узел упадет.  В отличии от 2PC в мастер-слейв схеме, которая не содержит шага для отмены операции в случае если операция была успешно выполнена на одних узлах а на других случился отказ и следовательно реплики оказались подвержены расхождениям.

2PC склонен к блокировкам, пока один узел отказал (участник или координатор) все операции блокируетются пока узел не востановится. Востановление зачастую возможно благодаря второй фазе, во время которой другие узлы информируют о состоянии системы. Заметим что 2PC допускает что данные в поостоянном хранилище никогда не будут потеряны и узлы никогда не откажут навсегда. Потери данных остаются возможными если данные в постоянном хранилище будут повреждены при падении.

Детали процедуры востановления после сбоев узлов довольно сложны и мы не будем вдаватся в специфику. Главные задачи это обеспечить надежную запись на диск(например записывать все на диск не кэшируя) и The major tasks are ensuring that writes to disk are durable (e.g. flushed to disk rather than cached) и убедиться, что были приняты правильные решения для востанновления (например изучив исход раунда востановить или отменить локальные изменения).

Как мы узнали в главе посвященной CAP теореме, 2PC это CA - он не устойчив к разделению. Модель отказов 2PC не включает в себя разделения сети; предписанный им способ востановится после отказа узла это подождать пока не востановится сеть. То есть несуществует безопасного способа выдвинуть нового координатора если один упал; необходимо ручное воздействие. 2PC также довольно чувствителен к задержкам, так как это N-из-N подход к записи когда запись не может продолжатся пока самый медленный узел не подтвердит ее.

2PC это достойный компромисс между производительностью и отказоустойчивостью, поэтому он так популярен в классических реляционных базах данных. Однако, более новые системы зачастую используют алгоритмы консенсуса устойчивые к разделению, поскольку такой алгоритм может обсепечить востанновление при временных разделениях сети, а также меньшую чувствительность к задержкам между узлами.

Давайте взглянем на алгоритмы консенсуса устойчивые к разделению.

## Алгоритмы консенсуса устойчивые к разделению

Алгоритмы консенсуса устойчивые к разделению ушли насколько это возможно далеко от устойчивых к отказам алгоритмов представляющих консистентность без расхождений. Следующий класс алгоритмов устойчивым к отказам: алгоритмы устойчивые к [произвольным (Византийским) отказам](http://en.wikipedia.org/wiki/Byzantine_fault_tolerance); они включают в себя отказы проявляющиеся в некорректном поведении узлов. Такие алгоритмы редко используются в коммерческих системах, потому что они более дорогие и сложные для реализации и запуска - и следовательно мы не будем обсуждать их.

Когда обсуждение доходит до алгоритмов консенсуса устойчивых к разделениям, многие вспоминают широко известный алгоритм Paxos. Однако общеизвестно что он довольно сложен для реализации и обьяснения, так что мы сфокусируемся на Raft, не так давно созданым алгоритмом(начало 2013) с целью быть более простым в понимании и реализации. Давайте сперва взглянем на сетевое разделение и главные характеристики алгоритмов консенсуса устойчивых к разделениям.

### Что такое сетевое разделение?

Сетевое разделение это исчезновение связи по сети к одному или нескольким узлами. Узлы при этом остаются активны и даже могут обслуживать запросы клиентов на своей стороне сетевого раздела. Как мы узнали раньше - во время дискуссии о CAP теореме - сетевые разделения происходят и не все системы могут полноценно обрабатывать их.

Разделения сети коварны потому что во время сетевого разделения не возможно различать упавший узел от узла который отделен от остальных упашей сетью. Если происходит сетевое разделение, и при этом все узлы активны, тогда система разделяется на две партиции которые одновременно активн. 2 диаграммы ниже иллюстрируют как сетевое разделение может быть похожим на падение узла.

Система с 2 узлами, при падении узла и при разделении сети:

<img src="images/system-of-2.png" alt="replication" style="max-height: 100px;">

Система с 3 узлами, при падении узла и при разделении сети:

<img src="images/system-of-3.png" alt="replication"  style="max-height: 130px;">

Система обеспечивающая согласованность без расхождений должна иметь способ отличать такие ситуации: иначе она будет разбита на две одинаковые системы которые могут расходится и система перестанет поддерживать иллюзию работы в единственном экземпляре.

Устойчивость к сетевым разделениям для систем обеспечивающих согласованность без расхождений, требует чтобы во время сетевого разделения только одна часть системы была активной, так как работе разделенной системе не возможно предотвратить расхождения(вспоминаем CAP теорему).

### Решения основанные на большинстве

Поэтому алгоритмы консенсуса устойчивые к разделению основаны на голосе большинства. Требование большинства узлов - в отличии от всех узлов (как в 2PC) - для согласия на обновление позволяет меньшинству узлов быть упавшими недоступными изза разделения сети или очень медленными. Пока `(N/2 + 1)-из-N` узлов работоспобны и доступны система может продолжать работу.

Алгоритмы консенсуса устойчивые к разделению используют нечетное числов узлов (например, 3, 5 или 7). Используя только 2 узла невозможно определить абсолютное большинство. Для примера, если у нас 3 узла, тогда система устойчива к отказу одного узла, если узлов 5 - тогда уже к падению двух.

Когда происходит разделение сети, партиции ведут себя асимметрично. Одна будет содержать большинство узлов. Меньшинство прекратит обрабатывать операции чтобы предотвратить расхождения пока сеть не востанновится, но партиция с большинством участников останется активной. Это обеспечивает единственную копию системы которая остается активной.

Большинство также полезно потому что оно устойчиво к несогласию узлов: если есть искажения или отказы, тогда узлы могут проголосовать по-разному. Однако, поскольку может быть только одно решение принятое большинством, временное несогласие может заблокировать принятие окончательного решения(отказ от свойства живучести) но не может нарушить критерий целостности "единой копии" (свойство корректности).

### Роли

Существует два способа выстраивать систему: либо все узлы имеют одни и теже обязанности, либо разные узлы могут иметь разные роли.

Алгоритмы консенсуса для репликации как правило разделяют роли между узлами. Наличие одного зафиксированного лидера или мастер-сервера это оптимизация которая делает системы более эффективной, так как мы знаем, что все обновления должны пройти через этот сервер. Узлы которые не являются лидером должны передавать запросы, пришедшие к ним, лидеру.

Заметим что наличие различных ролей не исключает востановления системы после отказа лидера(или другого узла). То что роли закреплены за узлами не означает, что при востановлении после сбоя может произойти переназначение ролей(например при помощи фазы выдвижения лидера). Узлы могу повторно использовать результат выбора лидера повторно пока не произойдет новый сбой узлов или сетевой раздел.

И Paxos и Raft используют разные роли для разных узлов. Обычно, они определяют лидера(предлагающий узел ("proposer") в Paxos) который отвечает за координацию во время нормальной работы системы. Во это время остальные узлы это "последователи"(followers) (принимающие ("acceptors") или голосующие("voters") узлы в Paxos).

### Эпохи

Каждый период нормального выполнения операции и в Paxos и в Raft называется эпохой("epoch") ("term" в Raft).  Во время каждой эпохи только один узел назначен лидером(похожая система [используется в Японии](http://en.wikipedia.org/wiki/Japanese_era_name) где названия эпох сменяется вместе со сменой императоров).

<img src="images/epoch.png" alt="replication"  style="max-height: 130px;">

После успешного выбора, лидер будет координировать систему пока не кончится эпоха. Как показано на диаграмме выше(из публикации о Raft), некотрые выборы лидера могут неудастся, что вызовет немедленный конец эпохи.

Эпохи работают как логические часы позволяющие узлам определить когда узлы с устаревшими данными начинают сообщатся с ними - узлы бывшие в отделенной части или были выведены из эксплутуации будут иметь меньший номер эпохи чем текущее значение эпохи работающих узлов, соотвественно команды устаревших узлов будут игнорироватся.

### Смена лидера при помощи дуэли

Во время нормальной операции, алгоритм консенсуса устойчивый к разделению довольно прост. Как мы увидели раньше, если мы не беспокоимся об устойчивости мы можем просто использовать 2PC. Более сложным является обеспечение условия что как только решение путем консенсуса было принято оно не будет потеряно и протокол сможет корректно обрабатывать смену лидера в результате сетевого сбоя или падения узла.

Все узлы начинают как ведомые(followers); один узел выбирается в лидеры на старте. Во время нормальной работы систем, лидер поддерживает индикатор своего состояния("heartbeat") который позволяет ведомым узлам определить когда лидер упал или оказался отделенным.

Когда узел определяет что лидер неотвечает(или в случае старта работы системы - лидера просто нету), он переключается в промежуточное состояние ("кандидата" в Raft) в котором он увеличивает счетчик эпохи на один, иницирует выборы лидера и соревнуется за то чтобы стать лидером с другими кандидатами.

Для того чтобы быть избранным лидером, узел должен получить большинство голосов. Один способ это присвоить голоса простым присвоением их по приниципу "первым прибыл - первым получил; в этом способе лидер будет выбран в конечном итоге. Добавление случайного количества времени ожидания между попытками узлов быть конкурировать за место лидера будут сокращать число узлов которые вытаются быть выбранными одновременнно.

### Номерованные предложения в пределах эпохи

Во время каждой эпохи лидер предлагает одно значение для голосования в один момент времени. В пределах эпохи каждое предложение нумеруется уникальным возрастающим числом. Ведомые(избирающие / акцепторы) принимают первое предложение с определенным номером предположения.

### Нормальное проведение операций

Во время нормальной операции, все предложения проходят через лидера. Когда клиент отправляет предложение(например иницирует операцию обновления), лидер контактирует со всеми узлами объединяя их в кворум. Если нет конкурирующих предложений (на основе полученных ответов от ведомых узлов), лидер предлагает значение. Если большинство узлов принимают значение, следовательно данное значение считается принятым.

Так как возможно, что другой узел также возможно пытается выступать в качестве лидера, мы должны гарантировать что конгда одно предложение было принято, его значение никогда не будет изменено. Иначе предложение которое уже было принято может быть к примеру отменено конкурирующим лидером. Лэмпорт говорит об этом так:

> P2: Если предложение со значением `v` было выбрано, тогда каждое предложение с большим номером должно предполагать что выбранное значение содержит `v`.

Для того чтобы это свойство было удовлетворено требуется чтобы и ведомые и предлагающие узлы были ограничены алгоритмом от изменений значений которые были приняты большинством. Заметим что "значение никогда не должно быть изменено" относится к значению времени одного исполнения(или запуска / экземпляра / решения) протокола. Типичные алгоритмы репликации запускаются несколько экземпляров алгоритма, но большинство обсуждений алгоритмов фокусируются на одном экземпляре алгоритма для большей простоты понимания. Мы хотим предотвратить историю решений от изменений или удаления.

Для соблюдения этого свойства, предлагающие узлы должны сперва спрашивать ведомые узлы о их (последнем) принятом предложении и значении этого предложения. Если предлагающий узел находит что предложение уже существует, тогда предложение просто отмечается как исполненное вместо того чтобы предлагать его. Лэмпорт говорит об этом так:

> P2b. Если предложение со значением `v` было выбрано, огда каждое предложение с большим номером выданное любым предлагающим узлом должно содержать значение `v`.

Более подробно:

> P2c. Для любого `v` и `n`, если предложение со значением `v` и номером `n` было предложено [лидером], тогда существует множество `S` содержащее большинство принимающих [ведомых] таким образом что либо (a) нет принимающих узлов в `S` которые приняли любое предложение с номером меньше `n` либо (b) `v` это значение последнего по номеру предложения среди всех предложений с номерами меньше чем `n` принятых ведомыми узлами из `S`.

Это ядро алгоритма Paxos, а также алгоритмов выросших из него. Значение которое было предложено не будет выбрано до второй фазы протокола. Предлагающие узлы должны иногда передавать предыдущее принятое решение для обеспечения корректности(это обеспечит утверждения b в P2c), пока они не достигают точки после которой они будут уверены что смогут заставить узлы принять предлагаемое значение (например такой точкой будет ситуация в утверждении a).

Если сущестует несколько предыдуших предложений, тогда будет предложено значение из предложения с наибольшим номером. Предлагающие могут попытатся навязать их значение только если совсем нет других конкурирующих предложений.

Для того чтобы обеспечить гарантию того что конкурирующее предложение не всплывет во время между опросами лидером(предлагающим) каждого акцептора о последнем значении, предлагающий узел говорит ведомым не принимать предложения с более низким номером предложения чем текущее.

Объединив все части вместе, принятие решения при помощи Paxos требует два раунда коммуникаций:

    [ Предлагающий узел ] -> Подготовка(n)                                [ Ведомые узлы ]
                 <- Обещание(n; предыдущий номер предложения и предыдущее значение если прошлое предложение было принято)

    [ Предлагающий ] -> ПринятьЗапрос(n, значение предлагающего узла или значение  [ Ведомых ]
                    ассоцированное с наибольшим по номеру предложением
                    полученным от фолловеров)
                    <- Принято(n, значение)

Стадия подготовки позволяет предлагающему узлу узнать о любых конкурирующих или предыдущих предложениях. Во второй фазе будет предложено либо новое значение либо предыдущие принятое значение. В некотрых случаях - таких как когда активны два предлагающих узлах в одно и тоже время(дуэль); когда сообщения потеряны; или большинство узлов отказали - тогда не одно предложение не будет принято большинством. Однако это приемлимо, так как правило принятия решения гласит что предложенное значение сходится к одному значению (с наибольшим номером предложения в прошлой попытке).

Действительно, в соотвествии с FLP теоремой, это лучшее что мы можем получить: алгоритмы которые решают проблему консенсуса должны пожертвовать либо корректностью либо живучестью если нет гарантий на длительность доставки сообщений. Paxos жертвует живучестью: принятие решения может быть отложено на неопределенный срок до того времени пока не будет конкурирующих лидеров, и большинство узлов примет значение. Это предпочтительнее нарушения гарантий корректности.

Конечно, реализация этого алгоритма намного сложнее чем может показатся. Есть много небольших проблем который добавляют довольно значительное количество кода даже в руках экспертов. Это такие проблемы как:

- практические оптимизации:
  - избавление от повторных выборов лидера при помощи "аренды" лидерства (а не проверок жизнеспособности лидера)
  - избежание повторных предложений сообщений когда система находится в стабильном состоянии и лидер не изменяется
- обеспечние надежности сохранения данных в стабильных хранилищах лидера и ведомых узлов в случае отказа хранлищ(к примеру разрушения диска)
- возможность безопасного включения/исключения из кластера(например базовый Paxos зависит от того факта что большинство всегда отделяется в одном узле, который не может обработать произвольные включения исключения из кластера)
- процедуры для доведения новой реплики до актуального состояния в случае после востановления или потери диска или при добавлении нового узла
- процедуры для бэкапинга и сборки мусора необходимых для обеспечения корректности после некотрого времени работы (компромисс между требованием к хранению данных и устойчивости при откаазах)

Публикация Google [Paxos Made Live](http://labs.google.com/papers/paxos_made_live.html) более детально освещает эти задачи.

## Алгоритмы консенсуса устойчивые к разделению: Paxos, Raft, ZAB

Надеюсь, вы теперь имеете представление о том как работают алгоритмы консенсуса устойчивые к разделению. Для лучшего понимания специфики конкретных алгоритмов необходимо прочитать статьи указанные в конце главе.

*Paxos*. Paxos это один из наиболее важных алгоритмов для случая когда создаются строго согласованные реплицированные системы с устойчивостью к разделению. Он используется во многих системах Google, включая [менеджер блокировок Chubby](http://research.google.com/archive/chubby.html) использующийся в [BigTable](http://research.google.com/archive/bigtable.html)/[Megastore](http://research.google.com/pubs/pub36971.html), Google File System а также [Spanner](http://research.google.com/archive/spanner.html).

Paxos назван в честь греческого острова Паксос, и был представлен Лесли Лэмпортом(Leslie Lamport) в публикации "Частичный парламент" в 1998. Он часто считается крайне трудным для реализации, и есть серия работ от компаний обладающих опытом в области распределенных систем, объясняюших многие практические детали(см. материалы для дальнейшего чтения). Возможно вам будет полезно прочитать комментарии Лэмпорта об этих проблемах [здесь](http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html#lamport-paxos) и [здесь](http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html#paxos-simple).

Большая часть проблем относится к тому факту что Paxos опичан в терминах одного раунда консенсуса, но актуальные работающие реализации обычно хотят запускать несколько раундов консенсуса для эффективности. Это привело к разработке многих  [расширений ядра протокола](http://en.wikipedia.org/wiki/Paxos_algorithm)  которые необходимо изучить тем кто заинтересован в построении системы основанной на Paxos. Кроме того, существуют дополнительные практические задачи такие как упростить изменение размеров кластера.

*ZAB*. ZAB - Zookeeper Atomic Broadcast протокол используемый в Apache Zookeeper. Zookeeper это система которая предоставляет примитивы для координации для распределенных систем, и используемая в многих Hadoop-ориентированных распределенных системах для координации (например [HBase](http://hbase.apache.org/), [Storm](http://storm-project.net/), [Kafka](http://kafka.apache.org/)). Zookeeper это в основном открытая версия Chubby. технически атомарный броадкастинг это другая проблема нежели чистая проблема консенсуса, но она также подпадает под категорию алгоритмов устойчивых к разделению предоставлящих строгую согласованность.

*Raft*. Raft это недавнее (2013) пополнение в группе алгоритмов. Разработанный чтобы быть проще в понимании, он представляет техе гарантии. В частности, различные части алгоритма лучше разделены и в документе также описан механизм для изменения членства в кластере. Существует реализация в [etcd](https://github.com/coreos/etcd) вдохновленнная ZooKeeper.

## Методы репликации с строгой согласованностью

В этой главе, мы как взглянули на методы репликации которые обеспечивают строгую согласованность. Начав с различий между синхронными и асинхронными действиями,  Starting with a contrast between synchronous work and asynchronous work, мы проложили себе путь к алгоритмам которые способны выдерживать более сложные проблемы. приведем некотрые харатеристики изученных алгоритмов:

#### Primary/Backup

- Единственный и неизменный мастер
- Репликация лога, реплики не учавствуют в операциях
- Нет границ для задержки репликации
- Неустойчиво к разделению
- Ручной/ситуационный фейловер(постанновление после отказа), не устойчиво к отказам, можно использовать как "живой бэкап"

#### 2PC

- Единогласное голосование: потвердить или отвергнуть
- Единственный мастер
- 2PC не переносит одновременного отказа координатора и узла во время подтверждения операции
- Не устойчив к разделению, чувствителен к задержкам

#### Paxos

- Голосование большинства
- Динамическое изменение лидера
- Устойчивость к n/2-1 одновременно отказавшим узлам, как часть протокола
- Малая чувствительность к задержкам

---

## Для дальнейшего чтения

#### Primary-backup and 2PC

- [Replication techniques for availability](http://scholar.google.com/scholar?q=Replication+techniques+for+availability) - Robbert van Renesse & Rachid Guerraoui, 2010
- [Concurrency Control and Recovery in Database Systems](http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx)

#### Paxos

- [The Part-Time Parliament](http://research.microsoft.com/users/lamport/pubs/lamport-paxos.pdf) - Leslie Lamport
- [Paxos Made Simple](http://research.microsoft.com/users/lamport/pubs/paxos-simple.pdf) - Leslie Lamport, 2001
- [Paxos Made Live - An Engineering Perspective](http://research.google.com/archive/paxos_made_live.html) - Chandra et al
- [Paxos Made Practical](http://scholar.google.com/scholar?q=Paxos+Made+Practical) - Mazieres, 2007
- [Revisiting the Paxos Algorithm](http://groups.csail.mit.edu/tds/paxos.html) - Lynch et al
- [How to build a highly available system with consensus](http://research.microsoft.com/lampson/58-Consensus/Acrobat.pdf) - Butler Lampson
- [Reconfiguring a State Machine](http://research.microsoft.com/en-us/um/people/lamport/pubs/reconfiguration-tutorial.pdf) - Lamport et al - changing cluster membership
- [Implementing Fault-Tolerant Services Using the State Machine Approach: a Tutorial](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.4762) - Fred Schneider

#### Raft and ZAB

- [In Search of an Understandable Consensus Algorithm](https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf), Diego Ongaro, John Ousterhout, 2013
- [Raft Lecture - User Study](http://www.youtube.com/watch?v=YbZ3zDzDnrw)
- [A simple totally ordered broadcast protocol](http://research.yahoo.com/pub/3274) - Junqueira, Reed
- [ZooKeeper Atomic Broadcast](http://research.yahoo.com/pub/3514)
