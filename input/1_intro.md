# %chapter_number%. Распределенные системы на высоком уровне

> Распределенное программирование это искуство решать теже самые проблемы которые вы можете решить на одном компьютере используя множество компьютеров.

Есть две базовых  задачи которые  должна выполнять любая система:

- хранение данных и
- вычисления

Распределенное программирование это искуство решать теже самые проблемы которые вы можете решить на одном компьютере используя множество компьютеров - обычно, потому что данные проблемы больше нельзя решить на одном компьютере.

Конечно в реальности ничто не заставляет вас использовать распределенные системы. Имея бесконечные деньги и бесконечное время на исследования, мы сможем обойтись без использования распределенных систем. А все вычисления и хранение всех данных могут быть произведены на некотрой "магической коробке" - бесконечно быстрое и надежная система находящаяся в одном экземпляре *за разработку которой вы вероятно заплатите кому то другому*.

Тем не менее, не многие люди имеют бесконечные ресурсы. Следовательно необходимо найти наиболее эффективную точку приложения средств на кривой затрат. В небольших масштабах, апгрейд оборудование является жизнеспособной стратегией. Однако, с увеличение размеров проблем вы столкнетесь с ситуацией когда апргейд оборудования на одному узле либо не решает проблемы либо стоит очень дорого. С этого момента, добро пожаловать в мир распределенных систем.

В нынешней ситуации лучшее решение сохранять стоимость оборудование  в середине ценового диапозона - до тех пор пока мы можем понижать стоимость решения проблем создавая распределенные отказоустойчивыые системы.

Вычисления могут получить приемущества от hi-end оборудование в той мере в которой оно может возместить потери производительности от медленного доступа в сеть быстрым доступом в память. Увеличение производительности путем приобретения hi-end обородувания ограничено задачами которые требуют большое число межсерверныз взаимодействий.

![cost-efficiency](images/barroso_holzle.png)

Как показывает картинка приведенная выше  [Barroso, Clidaras & Hölzle](http://www.morganclaypool.com/doi/abs/10.2200/S00516ED2V01Y201306CAC024) , разрыв производительности между hi-end системами и системами среднего уровня уменьшается с увеличением размера кластера в предположении равномерного доступа к памяти всех узлов.

В идеальном мире, добавление новой машины увеличивало бы производительность линейно. Но конечно это не возможно, так как существует некотрые накладные расходы на коммуникацию между узлами. Необходмио копировать данные с узла на узел а также координировать работу разных узлов. Именно поэтому стоит изучать распределенные алгоритмы - они предоставляют эффективные решения спечифических проблем когда это возможно либо снижает стоимость накладных расходов в случаев если избавиться от них совсем невзможно.

В центре внимания этого текста распределенное программирование и системы в реальной жизни, но не промышленно важный параметр - датацентры. К примеру, в книге не будет обсуждатся специфичные проблемы которые могут быть при какойто экзотической настройке сети или возникаюшие при доступе к разделенной памяти. Основной упор делается на изучение пространства возможных архитектурных решений а  не о оптимизации какой либо определенной системы - эта тема более подходит для более специализированного текста.

## Чего мы хотим достичь: Масштабируемость и другие хорошие вещи

Все начинается с решения проблемы с размерами(данных и нагрузок).

Многие вещи довольно тривиальны в малых масштабах - и становится крайне сложной при увеличении размеров или иных физических характеристик до определенного уровня. Легко поднять кусочек шоколада и тяжело поднять гору. Легко посчитать число людей в комнате и тяжело посчитать число людей в стране.

Этот факт является основой такого свойства как масштабируемость. Неформально говоря, в масштабируемой системе переходя от малого обьема к большим мы не должны получать постепенное ухудшение. Вот еще одно определение:

<dl>
  <dt>[Мастштабируемость](http://en.wikipedia.org/wiki/Scalability)</dt>
  <dd>способность системы, сети или процесса справляться с увеличением рабочей нагрузки (увеличивать свою производительность) при добавлении ресурсов.</dd>
</dl>

Что это могут быть за ресурсы? Вы можете измерять их в любом единицах измерения (число людей, использования электричества итд.). Но есть три интересных случая масштабируемости - каждый задействует определенный тип ресурсов:

- Мастштабируемость размера: добавление большего числа должно позволять увеличивать производительность линейно; увеличение размеров данных не должно увеличивать задержки обработки данных
- Гео-масштабируемость: возможность использовать несколько датацентров для сокрашения времени ответа юзеру, с учетом задержки кросс-дата-центр коммуникаций.
- Административная масштабируемость: добавление большего числа узлов не должно увеличивать количество администраторов необходимых для обслуживание всего парка машин.

Конечно в реальной системе рост происходит сразу в нескольких направлениях; каждый отдельный показатель фиксирует отдельный аспект роста.

Масштабируемая система продолжает удовлетворять потребности своих пользователей в то время как растет общий масштаб системы и данных. Есть два сопутсвующих аспекта - производительность и доступность - которые могут быть измерены раздичным способом.

### производительность (и отзывчивость)

<dl>
  <dt>[Производительность](http://en.wikipedia.org/wiki/Computer_performance)</dt>
  <dd>это характеристика количества полезной работы выполненой компьютерной системой в сравнение с потраченным ей временем и числом ресурсов.</dd>
</dl>

В зависимости от контекста она может включать в себя достижение одной или более целей:

- Малое время ответа/малое время задержки для отдельного участка работы
- Высокая пропускная способность(скорость обработки)
- Низкое потребление компьютерных

Однако существуют некотрые компромисы при достижении любой из этих целей. Для примера, система может достигнуть высокой производительности путем обработки информации большими партиями снижая рабочую нагрузку. Однако время ответа для одного запроса возрастет изза увеличения количества данных обрабатываемых за один запрос.

Я нахожу что низкая латентность(latency) системы - достижение малого времени ответа - наиболее важный аспект производительсности, так как он имеет строгую зависимость с физическими(скорее даже финансовыми) ограничениями. Т.е. задержки сильнее влиют на финансовые аспекты функционирования системы чем другие аспекты вопросы производительности.

Существое много определений понятия "латентность", но действительно просто определить его через этимологию данного слова:

<dl>
  <dt>Латентности</dt>
  <dd>Состояние в котором обьект латентнен; прерывание, период между началом чего либо и его возникновением.</dd>
</dl>

И что же мы понимаем под прилагательным "латентный"?

<dl>
  <dt>Латентный</dt>
  <dd>Существование в скрытом или неактивном режиме</dd>
</dl>

Это определение хорошо тем что оно подчеркивает как латентность определяет количество времени которое пройдет от того момента что либо произошло в системе до того момента когда это изменение станет видимым для наблюдателя(пользователя системы).

Для примера, представим что ты заразился вирусом который преврашает людей в зомби. Период латентности это время прошедшая с момента когда ты заразился до момента когда ты превратишься в зомби. Это и есть латентность: время требуюшееся для того чтобы чтото перешло из скрытого состояние в видимое.

Давайте предположим на минуту что наша распределенная система выполняет только высокоуровневые задачи: выполняет запрос который принимает все данные в системе и вычисляет по ним какой один результат. Другими словами, размышляйте о распределенной системе как о хранилище данных с возможностью выполнить некую детерминированную функцию от хранящихся в ней данных:

`результат = запрос(все данные в системе)`

Тогда, на величину задержки будет влиять не количество данных, а скорость с которой новые данные вступают в силу. Для примера, задержка может показывать как долго пользватели которые читают информацию из системы не увидять новых данных записанных другими пользователя.

Другой ключевой особенность данного определения является то что если в системе ничего не сменилось то период задержки будет равен нулю. Система в которой данные не изменяются не имеет проблем с отзывчивостью.

В распределенных системах есть минималььная задержка которую физически нельзя преодолеть: скорость света ограничивает скорость передачи информации а апаратные компоненты ограничивают скорость выполнения операций(в первую очередь диск и оперативная память но также и процессор).

Минимально возможная задержка зависит от того какое растояние надо преодолеть информации и типа запросов к данным.

### Доступность (и отказоустойчивость)

Второй аспект масштабируемых систем это доступность.

<dl>
  <dt>[Доступность](http://en.wikipedia.org/wiki/High_availability)</dt>
  <dd>время в течении которого система находится в состоянии работоспособности. Если пользователь не имеет доступа к системе - система не доступна. </dd>
</dl>

Распределенные системы позволяют нам достигать значений характеристик которые будет очень сложно достичь в одной системе. Для примера одна машина никогда не будет устойчива ко всему спектру отказов.

Распределенные системы позволяют брать ненадежные элементы и объединяя их получать надежную систему.

Системы которые не имеют избыточности надежны настолько насколько надежны их компоненты. Системы построенные с избыточностью могут быть устойчивы к частичным сбоям их компонентов.Избычтоность может выражатся в чем угодно - серверах, компонентах, датацентрах ит.

Формально, доступность это: `Доступность = время работы / (время работы + время отказа)`.

С технической точки зрения доступность системы обеспечивается в певую очередь высокой отказоустойчивостью. Поскольку вероятность отказа одного конкретного компонента системы увеличиваается с ростом числа компонентов система должна это компенсировать тем что не позволять увеличиваться вероятность отказа всей системы при росте числа компонентов.

Пример:

<table>
<tr>
  <td>Достуность %</td>
  <td>Суммарное время неработоспособности системы в год?</td>
</tr>
<tr>
  <td>90% ("одна девятка")</td>
  <td>Более месяца</td>
</tr>
<tr>
  <td>99% ("две девятки")</td>
  <td>более 4 дней</td>
</tr>
<tr>
  <td>99.9% ("три девятки")</td>
  <td>более 9 часов</td>
</tr>
<tr>
  <td>99.99% ("четыре девятки")</td>
  <td>более чем час</td>
</tr>
<tr>
  <td>99.999% ("5 девяток")</td>
  <td>~ 5 минут</td>
</tr>
<tr>
  <td>99.9999% ("6 девяток")</td>
  <td>~ 31 секунд</td>
</tr>
</table>


Доступность конечно  не определяется времемнем безотказной работы. Сервисы могут быть недоступны изза сетевых проблем либо каких либо проблем бизнесса (что конечно не проблема решаемая путем отказоустойчивости). Но не имею информации о всех аспектах будующего функционирования системы лучшее что мы можем сделать это разрабатывать системы закладывая в нее свойство отказоустойчивости.

Что мы понимаем под отказоустойчивостью?What does it mean to be fault tolerant?

<dl>
  <dt>Отказоустойчивость</dt>
  <dd>Способность системы вести себя заранее определенным образом в случае отказов</dd>
</dl>

Создание отказоустойчивых систем сводится к следующему: определить возможные отказы и разработать алгоритмы которые нечувстивительны к такого рода отказов. Вы не можете создать алгоритм не чувствительный к отказам которые вы не предустмотрели.

## Что мешает нам достичь этих целей(производительность и отказоустойчивость)?

Распределенные системы ограничены двумя факторами:

- число узлов (которое будет возрастать в соотвествии с возрастающими требованиями к хранию данных и их обработке)
- растояние между узлами (информация распространяется в лучшем случае со скоростью света)

Работая в этих ограничениях:

- при увеличении числа независимых узлов увеличивается вероятность отказа в системе (сокращая доступность и увеличивая административные расходы)
- при увеличении числа независимых узлов может возрастать необходимость в коммуникации между узлами (сокращая производительность при масштабировании системы)
- при увеличении растояния между узлами увеличевается задержка при коммуниции между удаленными узлами(сокращая производительность каждой конкретной операции)

Помимо ограничений физического характера существуют также ограничения выбраного нами дизайна системы.

Система должна гарантировать поддержание производительности и отказоустойчивости. Если рассуждать абстрактно, то вы можете думать о данных гарантиях, как о некоем "Соглашении об уровне услуг" (*англ. SLA*), которое отвечает на ряд вопросов. Если я запишу данные, как быстро они будут доступны в другом месте? После того как я записал данные, какие гарантии даются касательно их сохранности? Если я сделаю запрос к системе на вычисление, как быстро я буду получать результат? Если компоненты выйдут из строя, какое влияние это окажет на систему в целом?

Есть еще один критерий, который как правило не упоминается, но подразумевается - понятность. Он отвечает на вопрос: насколько понятны предлагаемые гарантии? К сожалению, не существует каких-то простых метрик, описывающих понятность.

Мне хотелось добавить "понятность" к физическим ограничениям. В конце концов, применительно к людям, это действительно физическое ограничение. Нам нужно время, чтобы вникнуть в суть предмета, если он одновременно включает в себя [больше непосредственно взаимодействующих частей, чем у нас пальцев](http://en.wikipedia.org/wiki/Working_memory#Capacity). Отсюда вытекает различие между двумя нежелательными поведениями системы "ошибкой" и "аномалией" - ошибкой назвают некорректное поведение, аномалия же в свою очередь, это неожиданное поведение. Хороший специалист должен быть готов, к тому, что аномалии могут происходить.

## Абстракции и модели

В этом месте в игру вступают абстракции и модели. Абстракции скрывают некоторые детали, которые непосредственно не влияют на решение задачи, позволяя управлять глобальными объектами. А модели максимально точно описывают ключевые свойства распределенной системы. В следующей главе будут рассмотрены следующие виды моделей:

- Модель системы (ассинхронная / синхронная)
- Модель отказов (crash-fail, partitions, Byzantine)
- Модель консистентности (строгая, причинная)

Хорошая абстракция облегчает работу с системой, путем манипулирования только ключевыми факторами, имеющими значение при достижении конкретной цели.

При наличии множества узлов, наше желание чтобы "система работала как единое целое" не всегда всегда оказывается правильным. Часто наиболее привычная модель (например, создание разделяемой памяти), является невыгодным.

Система, которая дает наиболее неопределенные гарантии имеет большую свободу действий, следовательно потенциально большую производительность. Но это так же потенциально более сложный объект для понимания. Людям проще думать о системе, как о едином целом, а не о наборе узлов.

One can often gain performance by exposing more details about the internals of the system. For example, in [columnar storage](http://en.wikipedia.org/wiki/Column-oriented_DBMS), the user can (to some extent) reason about the locality of the key-value pairs within the system and hence make decisions that influence the performance of typical queries. Systems which hide these kinds of details are easier to understand (since they act more like single unit, with fewer details to think about), while systems that expose more real-world details may be more performant (because they correspond more closely to reality).

Several types of failures make writing distributed systems that act like a single system difficult. Network latency and network partitions (e.g. total network failure between some nodes) mean that a system needs to sometimes make hard choices about whether it is better to stay available but lose some crucial guarantees that cannot be enforced, or to play it safe and refuse clients when these types of failures occur.

The CAP theorem - which I will discuss in the next chapter - captures some of these tensions. In the end, the ideal system meets both programmer needs (clean semantics) and business needs (availability/consistency/latency).

## Design techniques: partition and replicate

The manner in which a data set is distributed between multiple nodes is very important. In order for any computation to happen, we need to locate the data and then act on it.

There are two basic techniques that can be applied to a data set. It can be split over multiple nodes (partitioning) to allow for more parallel processing. It can also be copied or cached on different nodes to reduce the distance between the client and the server and for greater fault tolerance (replication).

> Divide and conquer - I mean, partition and replicate.

The picture below illustrates the difference between these two: partitioned data (A and B below) is divided into independent sets, while replicated data (C below) is copied to multiple locations.

![Partition and replicate](images/part-repl.png)

This is the one-two punch for solving any problem where distributed computing plays a role. Of course, the trick is in picking the right technique for your concrete implementation; there are many algorithms that implement replication and partitioning, each with different limitations and advantages which need to be assessed against your design objectives.

### Partitioning

Partitioning is dividing the dataset into smaller distinct independent sets; this is used to reduce the impact of dataset growth since each partition is a subset of the data.

- Partitioning improves performance by limiting the amount of data to be examined and by locating related data in the same partition
- Partitioning improves availability by allowing partitions to fail independently, increasing the number of nodes that need to fail before availability is sacrificed

Partitioning is also very much application-specific, so it is hard to say much about it without knowing the specifics. That's why the focus is on replication in most texts, including this one.

Partitioning is mostly about defining your partitions based on what you think the primary access pattern will be, and dealing with the limitations that come from having independent partitions (e.g. inefficient access across partitions, different rate of growth etc.).

### Replication

Replication is making copies of the same data on multiple machines; this allows more servers to take part in the computation.

Let me inaccurately quote [Homer J. Simpson](http://en.wikipedia.org/wiki/Homer_vs._the_Eighteenth_Amendment):

> To replication! The cause of, and solution to all of life's problems.

Replication - copying or reproducing something - is the primary way in which we can fight latency.

- Replication improves performance by making additional computing power and bandwidth applicable to a new copy of the data
- Replication improves availability by creating additional copies of the data, increasing the number of nodes that need to fail before availability is sacrificed

Replication is about providing extra bandwidth, and caching where it counts. It is also about maintaining consistency in some way according to some consistency model.

Replication allows us to achieve scalability, performance and fault tolerance. Afraid of loss of availability or reduced performance? Replicate the data to avoid a bottleneck or single point of failure. Slow computation? Replicate the computation on multiple systems. Slow I/O? Replicate the data to a local cache to reduce latency or onto multiple machines to increase throughput.

Replication is also the source of many of the problems, since there are now independent copies of the data that has to be kept in sync on multiple machines - this means ensuring that the replication follows a consistency model.

The choice of a consistency model is crucial: a good consistency model provides clean semantics for programmers (in other words, the properties it guarantees are easy to reason about) and meets business/design goals such as high availability or strong consistency.

Only one consistency model for replication - strong consistency - allows you to program as-if the underlying data was not replicated. Other consistency models expose some internals of the replication to the programmer. However, weaker consistency models can provide lower latency and higher availability - and are not necessarily harder to understand, just different.

---

## Further reading

- [The Datacenter as a Computer - An Introduction to the Design of Warehouse-Scale Machines](http://www.morganclaypool.com/doi/pdf/10.2200/s00193ed1v01y200905cac006) - Barroso &  Hölzle, 2008
- [Fallacies of Distributed Computing](http://en.wikipedia.org/wiki/Fallacies_of_Distributed_Computing)
- [Notes on Distributed Systems for Young Bloods](http://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/) - Hodges, 2013
